### Pretraining-Based Natural Language Generation for Text Summarization

#### Abstract

This paper propose a pretraing-based encoder-decoder framwork, which can generate the output sequence based on the input sequence in a two-stage manner.

Encoder: encode the input sequence into context representations using BERT. Decoder: two stages, in the first stage, use a Transformer-based decoder to generate a draft output sequence. In the second stage, mask each word of the draft sequence and feed it to BERT, then by combining the input sequence and the draft representation generated by BERT, use a Transformer-based decoder to predict the refined word for each masked position.

It is the first method which applies the BERT into text generation tasks.

Task: text summarization task

Experimental datasets: CNN/Daily Mail, New York Times

#### Introduction

Text summarization generates summaries from input documents while keeping salient information. It is an important task and can be applied to several real-world applications. There are two main text summarization techniques: extractive and abstractive. Extractive summarization generates summary by selecting salient sentences or phrases from the source text, while abstractive methods paraphrase and restructure sentences to compose the summary. This paper focus on abstractive summarization as it is more flexible and thus can generate more diverse summaries.

many approachs are based on neural sequence-to-sequence framework.
