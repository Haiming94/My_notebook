### Pretraining-Based Natural Language Generation for Text Summarization

#### Abstract

This paper propose a pretraing-based encoder-decoder framwork, which can generate the output sequence based on the input sequence in a two-stage manner.

Encoder: encode the input sequence into context representations using BERT. Decoder: two stages, 
