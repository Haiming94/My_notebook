### Pretraining-Based Natural Language Generation for Text Summarization

#### Abstract

This paper propose a pretraing-based encoder-decoder framwork, which can generate the output sequence based on the input sequence in a two-stage manner.

Encoder: encode the input sequence into context representations using BERT. Decoder: two stages, in the first stage, use a Transformer-based decoder to generate a draft output sequence. In the second stage, mask each word of the draft sequence and feed it to BERT, then by combining the input sequence and the draft representation generated by BERT, use a Transformer-based decoder to predict the refined word for each masked position.

It is the first method which applies the BERT into text generation tasks.

Task: text summarization task

Experimental datasets: CNN/Daily Mail, New York Times
